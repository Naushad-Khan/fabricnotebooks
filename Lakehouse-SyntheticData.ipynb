{"cells":[{"cell_type":"code","execution_count":null,"id":"1eb5f340-daf3-49a6-9d9c-9b6e797cd05a","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","\n","%pip install dbldatagen --quiet \n","%pip install jmespath --quiet"]},{"cell_type":"code","execution_count":null,"id":"81b69840-e698-43fa-bc3d-1f013b193a93","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import dbldatagen as dg\n","import dbldatagen.distributions as dist\n","import random\n","from pyspark.sql.types import FloatType, StructType, StructField,  StringType, IntegerType\n","import pandas as pd\n","from pyspark.sql.functions import col, last_day, dayofweek, year, month, date_format\n","\n","\n","country_codes = ['CN', 'US', 'FR', 'CA', 'IN', 'JM', 'IE', 'PK', 'GB', 'IL', 'AU', 'SG', 'ES', 'GE', 'MX', 'ET', 'SA', 'LB', 'NL']\n","\n","colors = ['White','Black','Grey','Silver','Orange','Yellow','Blue','Brown','Gold','Silver Grey','Pink','Red','Green','Transparent','Purple','Azure']\n","categories = ['Audio','TV and Video','Computers','Cell phones','Music, Movies and Audio Books','Home Appliances','Cameras and camcorders','Games and Toys']\n","categories_weights = [1000,900,500,400,100,10,300,90]\n","subCategories = ['MP4&MP3','Home Theater System','Projectors & Screens','Computers Accessories','Home & Office Phones','Movie DVD','Washers & Dryers','Microwaves','Water Heaters','Coffee Machines','Air Conditioners','Digital SLR Cameras','Touch Screen Phones','Cell phones Accessories','Camcorders','VCD & DVD','Car Video','Boxed Games','Bluetooth Headphones','Digital Cameras','Smart phones & PDAs','Televisions','Laptops','Desktops','Monitors','Printers, Scanners & Fax','Refrigerators','Lamps','Fans','Cameras & Camcorders Accessories','Recording Pen','Download Games']\n","\n","start_date = '2020-01-01 00:00:00'\n","end_date = '2024-12-31 00:00:00'\n","\n","customer_rows = 1000\n","store_rows = 100\n","product_rows = 1000\n","sales_rows = 10000\n","\n","num_fact_tables = 3\n","num_random_tables = 10\n","num_columns_random_tables = 15\n","random_table_prefix = \"zRandomTable\"\n","\n","listOfTables = []\n","\n","# Customer table\n","\n","dataSpec = (\n","    dg.DataGenerator(spark, name=\"customerDataset\", rows=customer_rows)\n","    .withColumn(\"customerId\", IntegerType(),expr=\"id + 1\")\n","    .withColumn(\"customer\", template=r\"\\w \\w\")\n","    .withColumn(\"email\", template=r\"\\w.\\w@\\w.com\")    \n","    .withColumn(\"country\", StringType(), values=country_codes, random=True, distribution=dist.Gamma(1.0, 2.0))\n","    .withColumn(\"birthday\", \"date\", data_range=dg.DateRange(\"1942-01-01 00:00:00\", \"2010-10-06 11:55:00\", \"days=3\"), random=True)\n","    .withColumn(\"gender\", StringType(), values=[\"male\", \"female\"], random=True)\n","    )\n","\n","df = dataSpec.build()\n","\n","listOfTables.append({'name': 'customer', 'data': df})\n","\n","# store table\n","\n","dataSpec = (\n","    dg.DataGenerator(spark, name=\"storeDataset\", rows=store_rows)\n","    .withColumn(\"storeId\", IntegerType(),expr=\"id + 1\")\n","    .withColumn(\"store\", template=r\"\\w \\w \\w\")\n","    .withColumn(\"openDate\", \"date\", data_range=dg.DateRange(\"1942-01-01 00:00:00\", \"2010-10-06 11:55:00\", \"days=3\"), random=True)        \n","    .withColumn(\"status\", StringType(), values=[\"open\", \"closed\"], weights=[100,10], random=True)\n","    )\n","\n","df = dataSpec.build()\n","\n","listOfTables.append({'name': 'store', 'data': df})\n","\n","# product table\n","\n","dataSpec = (\n","    dg.DataGenerator(spark, name=\"productDataset\", rows=product_rows)    \n","    .withColumn(\"productId\", IntegerType(),expr=\"id + 1\")\n","    .withColumn(\"product\", template=r\"\\w \\w \\w\")    \n","    .withColumn(\"color\", StringType(), values=colors, random=True, distribution=dist.Gamma(1.0, 2.0))\n","    .withColumn(\"category\", StringType(), values=categories, weights=categories_weights, random=True)\n","    .withColumn(\"unitPrice\", 'decimal(10,2)', minValue=1, maxValue=100, random=True)\n","    .withColumn(\"weight\", 'decimal(10,2)', minValue=1, maxValue=50, step=0.1, random=True)    \n","    )\n","\n","df = dataSpec.build()\n","\n","listOfTables.append({'name': 'product', 'data': df})\n","\n","# sales table\n","\n","for i in range(1, num_fact_tables+1):\n","\n","    dataSpec = (\n","        dg.DataGenerator(spark, name=\"salesDataset\", rows=sales_rows, randomSeed  = i)\n","        .withColumn(\"salesId\", IntegerType(),expr=\"id + 1\")\n","        .withColumn(\"customerId\", IntegerType(),  minValue=1, maxValue=customer_rows, random=True)\n","        .withColumn(\"productId\", IntegerType(),  minValue=1, maxValue=product_rows, random=True)\n","        .withColumn(\"storeId\", IntegerType(),  minValue=1, maxValue=store_rows, random=True)    \n","        .withColumn(\"orderDate\", \"date\", data_range=dg.DateRange(start_date, end_date, \"days=10\"), random=True)\n","        .withColumn(\"shippingDate\",\"date\", expr=\"date_add(orderDate, cast(floor(rand() * 20 + 1) as int))\", baseColumn=[\"orderDate\"])\n","        .withColumn(\"quantity\", IntegerType(),  minValue=0, maxValue=500, random=True, distribution=dist.Gamma(1.0, 2.0))    \n","        .withColumn(\"price\", 'decimal(10,2)', minValue=1, maxValue=10, random=True, distribution=dist.Gamma(1.0, 2.0))\n","        .withColumn(\"amount\",\"decimal(10,2)\", expr=\"quantity*price\", baseColumn=[\"quantity\", \"price\"])\n","        )    \n","\n","    df = dataSpec.build()\n","        \n","    if (i == 1):\n","        listOfTables.append({'name': f\"sales\", 'data': df})\n","    else:\n","        listOfTables.append({'name': f\"sales_{i}\", 'data': df})\n","\n","# date dimension\n","\n","date_df = pd.date_range(start=start_date, end=end_date).to_frame(index=False, name='Date')\n","date_df['Date'] = date_df['Date'].astype(str)\n","\n","dfCalendarData = spark.createDataFrame(date_df)\n","\n","dfCalendarData = dfCalendarData.withColumn('date', col('Date').cast('date'))\n","dfCalendarData = dfCalendarData.withColumn('dateID', date_format(col('Date'),\"yyyyMMdd\").cast('integer'))\n","dfCalendarData = dfCalendarData.withColumn('monthly', date_format(col('Date'),\"yyyy-MM-01\").cast('date'))\n","dfCalendarData = dfCalendarData.withColumn('month', date_format(col('Date'),\"MMM\"))\n","dfCalendarData = dfCalendarData.withColumn('monthYear', date_format(col('Date'),\"MMM yyyy\"))\n","dfCalendarData = dfCalendarData.withColumn('monthOfYear', month(col('Date')))\n","dfCalendarData = dfCalendarData.withColumn('year', year(col('Date')))\n","dfCalendarData = dfCalendarData.withColumn('dayOfWeekNum', dayofweek(col('Date')))\n","dfCalendarData = dfCalendarData.withColumn('dayOfWeek', date_format(col('Date'),\"EE\"))\n","\n","listOfTables.append({'name': 'calendar', 'data': dfCalendarData})\n","\n","# random tables\n","\n","for i in range(1, num_random_tables+1):\n","\n","    randomTableRowCount = random.randint(100, 1000)     \n","\n","    dataSpec = (\n","        dg.DataGenerator(spark, name=\"test_data_set1\", rows=randomTableRowCount, randomSeed = i)\n","        .withIdOutput()\n","        .withColumn(\n","            \"r\",\n","            FloatType(),\n","            expr=\"floor(rand() * 350) * (86400 + 3600)\",\n","            numColumns=num_columns_random_tables,\n","        )\n","        .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200)\n","        .withColumn(\"code2\", \"integer\", minValue=0, maxValue=10, random=True)\n","        .withColumn(\"code3\", StringType(), values=[\"online\", \"offline\", \"unknown\"])\n","        .withColumn(\"code4\", template=r\"\\w\")    \n","        .withColumn(\n","            \"code5\", StringType(), values=[\"a\", \"b\", \"c\", \"d\", \"e\"], random=True, percentNulls=0.05\n","        )\n","        .withColumn(\n","            \"code6\", \"string\", values=[\"a\", \"b\", \"c\"], random=True, weights=[9, 1, 1]\n","        )\n","    )\n","\n","    df = dataSpec.build()\n","\n","    listOfTables.append({'name': f\"{random_table_prefix}_{i}\", 'data': df})\n","\n","# Save to Lakehouse\n","\n","for table in listOfTables:\n","\n","    tableName = table['name']\n","\n","    print(f\"Saving table '{tableName}'\")\n","\n","    df = table['data']\n","\n","    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tableName)   "]},{"cell_type":"code","execution_count":null,"id":"6987999d-673c-4991-8e48-1663ff24302a","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["\n","for table in listOfTables:\n","    tableName = table['name']\n","\n","    spark.sql(f\"SELECT count(*) as {tableName} FROM {tableName}\").show()"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"a7dfc75f-c2ba-422f-9ca5-4f42f9b5147d","default_lakehouse_name":"MyLakehouse","default_lakehouse_workspace_id":"8a7ac012-d6cf-4d1b-bcdc-c536c7efead0","known_lakehouses":[{"id":"a7dfc75f-c2ba-422f-9ca5-4f42f9b5147d"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
